{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpTD4ymHVu8tqJWFLcys9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yliang412/restaurant-recommender/blob/main/restaurant_recommender_mf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant Recommender - Matrix Factorization"
      ],
      "metadata": {
        "id": "zD05KJ25VCAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "bNYJb6G1VHyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1wakiFSU0gg",
        "outputId": "53408300-4caa-493a-94df-73b08b4ec9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# Replace this with the path in your drive if you created the shortcut somewhere else\n",
        "path_in_my_drive = 'gdrive/My Drive/ml-final-project(mzhang3,yuchenl3,kehaoc)'\n",
        "\n",
        "# # Uncomment if you need the original dataset\n",
        "# !unzip 'gdrive/My Drive/ml-final-project(mzhang3,yuchenl3,kehaoc)/data/yelp-dataset.zip' -d 'data'\n",
        "\n",
        "# Load the processed data frames\n",
        "!cp -r 'gdrive/My Drive/ml-final-project(mzhang3,yuchenl3,kehaoc)/processed_data' ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "82imSMd9VVDv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "15ZSlkhYVOvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yelp_df(table_name: str, nrows=None):\n",
        "    \"\"\"Load an original yelp data frame.\"\"\"\n",
        "\n",
        "    chunks = pd.read_json(f'data/yelp_academic_dataset_{table_name}.json', lines=True, chunksize=100000, nrows=nrows)\n",
        "    return pd.concat(chunks)\n",
        "\n",
        "\n",
        "def load_processed_df(table_name: str, from_gdrive=False):\n",
        "    \"\"\"Load a processed data frame.\"\"\"\n",
        "    \n",
        "    path = f'{path_in_my_drive}/processed_data/{table_name}.parquet.gzip' if from_gdrive \\\n",
        "        else f'processed_data/{table_name}.parquet.gzip'\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "\n",
        "def save_processed_df(df: pd.DataFrame, table_name: str):\n",
        "    \"\"\"Save a processed dataframe to google drive.\"\"\"\n",
        "\n",
        "    path = f'{path_in_my_drive}/processed_data/{table_name}.parquet.gzip'\n",
        "    with open(path, 'wb') as f:\n",
        "        df.to_parquet(f, compression='gzip')"
      ],
      "metadata": {
        "id": "nTLSCTrRVM1L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ratings_preprocess(nrows=None):\n",
        "    \"\"\"Preprocess the ratings data frame to be used for Matrix Factorization.\"\"\"\n",
        "\n",
        "    rating_df = load_processed_df('user_restaurant_rating')\n",
        "    if nrows:\n",
        "        rating_df = rating_df.loc[1:nrows, :]\n",
        "    \n",
        "    rating_df[\"user_id\"] = rating_df[\"user_id\"].astype(\"category\")\n",
        "    rating_df[\"business_id\"] = rating_df[\"business_id\"].astype(\"category\")\n",
        "    rating_df[\"stars\"] = rating_df[\"stars\"].astype(\"float32\")\n",
        "\n",
        "    cat_columns = rating_df.select_dtypes(['category']).columns\n",
        "    rating_df[cat_columns] = rating_df[cat_columns].apply(lambda x: x.cat.codes)\n",
        "    rating_numeric_df = rating_df[['user_id', 'business_id', 'stars']]\n",
        "    return rating_numeric_df\n",
        "\n",
        "def ratings_train_test_split(rating_numeric_df, test_size=0.2):\n",
        "    \"\"\"Create two disjoint matrix based on the user id.\"\"\"\n",
        "\n",
        "    all_users = rating_numeric_df['user_id'].unique()\n",
        "    train_users, test_users = train_test_split(all_users, test_size=test_size)\n",
        "\n",
        "    n_users = all_users.shape[0]\n",
        "    n_restaurants = rating_numeric_df['business_id'].unique().shape[0]\n",
        "\n",
        "    train_mat = sp.csr_matrix((rating_numeric_df['stars'], (rating_numeric_df['user_id'], rating_numeric_df['business_id'])), shape=(n_users, n_restaurants))\n",
        "    test_mat = train_mat.copy()\n",
        "    train_mat[train_users] = 0\n",
        "    train_mat.eliminate_zeros()\n",
        "    train_mat = train_mat.tocoo()\n",
        "    test_mat[test_users] = 0\n",
        "    test_mat.eliminate_zeros()\n",
        "    test_mat = test_mat.tocoo()\n",
        "    return train_users, train_mat, test_users, test_mat"
      ],
      "metadata": {
        "id": "bcXS0TFlVyEz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YelpRatingsDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, mat):\n",
        "        \"\"\"Yelp dataset initializer.\n",
        "\n",
        "        Arguments:\n",
        "            - mat: user-restaurant rating matrix.\n",
        "        \"\"\"\n",
        "        self.mat = mat\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the length of the YelpRatingsDataset object.\n",
        "        \n",
        "        Returns:\n",
        "            - length: number of unique user-restaurant pairs in the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.mat.nnz\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns a user-restaurant pair and the rating of the restaurant given by the user.\n",
        "        \n",
        "        Arguments:\n",
        "            - index: the index of the sample, must be in [0, self.__len__()).\n",
        "        Returns:\n",
        "            - user_index: user index in the sample.\n",
        "            - restaurant_index: restaurant index in the sample\n",
        "            - rating: the rating of the restaurant given by the user.\n",
        "        \"\"\"\n",
        "\n",
        "        user_index = self.mat.row[index]\n",
        "        restaurant_index = self.mat.col[index]\n",
        "        rating = self.mat.data[index]\n",
        "\n",
        "        return (user_index, restaurant_index), rating"
      ],
      "metadata": {
        "id": "fhGVC9yUWP_Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation"
      ],
      "metadata": {
        "id": "68jTpdgaXPtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MatrixFactorization(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "        self.user_factors = torch.nn.Embedding(n_users, n_factors, sparse=True)\n",
        "        self.item_factors = torch.nn.Embedding(n_items, n_factors, sparse=True)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
      ],
      "metadata": {
        "id": "PXDe2rvRXgQT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommendationPipeline:\n",
        "    \n",
        "    def __init__(self, mat_shape, n_factors=20, lr=1e-6):\n",
        "        n_users, n_items = mat_shape\n",
        "        self.model =  MatrixFactorization(n_users, n_items, n_factors=n_factors)\n",
        "        self.loss_func = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
        "       \n",
        "    \n",
        "    def train(self, dl):\n",
        "        for (user_id, restaurant_id), rating in dl:\n",
        "            # Set gradients to zero\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Predict and calculate loss\n",
        "            prediction = self.model(user_id, restaurant_id)\n",
        "            loss = self.loss_func(prediction, rating)\n",
        "\n",
        "            # Backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            self.optimizer.step()\n",
        "    \n",
        "    def model(self):\n",
        "        return self.model\n",
        "\n",
        "    \n",
        "    def validate(self, dl):\n",
        "        losses = []\n",
        "        for (user_id, restaurant_id), rating in dl:\n",
        "            # Predict and calculate loss\n",
        "            prediction = self.model(user_id, restaurant_id)\n",
        "            loss = self.loss_func(prediction, rating)\n",
        "            # TODO: calculate like the average?\n",
        "            losses.append(loss)\n",
        "\n",
        "        return torch.Tensor(losses)"
      ],
      "metadata": {
        "id": "puHWXprpaQkF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline\n",
        "rating_numeric_df = ratings_preprocess(nrows=10_000)\n",
        "train_users, train_mat, test_users, test_mat = ratings_train_test_split(rating_numeric_df, test_size=0.2)\n",
        "train_dl, test_dl = DataLoader(YelpRatingsDataset(train_mat)), DataLoader(YelpRatingsDataset(test_mat))\n",
        "\n",
        "pipeline = RecommendationPipeline(train_mat.shape, n_factors=20, lr=1e-6)\n",
        "pipeline.train(train_dl)\n",
        "losses = pipeline.validate(test_dl)\n",
        "torch.mean(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mAEhtgvfBgJ",
        "outputId": "7689f567-93e4-4864-849c-3a158443072e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(36.7009)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}